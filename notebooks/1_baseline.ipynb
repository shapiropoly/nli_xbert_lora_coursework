{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras-nlp\n",
    "!pip install seaborn"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)"
   ],
   "id": "f05394a2d839d79f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"replicas:\", strategy.num_replicas_in_sync)"
   ],
   "id": "2d91197497b2e0b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Num replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "print(\"Available devices:\")\n",
    "for d in tf.config.list_physical_devices():\n",
    "    print(d)"
   ],
   "id": "e18a612a3f53aa64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATA_DIR = '/kaggle/input/contradictory-my-dear-watson/'\n",
    "\n",
    "RESULT_DICT = {\n",
    "    0 : \"entailment\",\n",
    "    1 : \"neutral\",\n",
    "    2 : \"contradiction\"\n",
    "}\n",
    "\n",
    "for dirname, _, filenames in os.walk(DATA_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "id": "3861ed7e528b1313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "df_train.head()"
   ],
   "id": "a674baf57581f893"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "df_test.head()"
   ],
   "id": "1c8e5f353c580313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def display_pair_of_sentence(x):\n",
    "    print( \"Premise : \" + x['premise'])\n",
    "    print( \"Hypothesis: \" + x['hypothesis'])\n",
    "    print( \"Language: \" + x['language'])\n",
    "    print( \"Label: \" + str(x['label']))\n",
    "    print()\n",
    "\n",
    "df_train.head(10).apply(lambda x : display_pair_of_sentence(x), axis=1)\n",
    "\n",
    "df_train.shape"
   ],
   "id": "43c5162abd322704"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.despine()\n",
    "ax = sns.countplot(data=df_train,\n",
    "                   y=\"label\",\n",
    "                   order = df_train['label'].value_counts().index)\n",
    "\n",
    "abs_values = df_train['label'].value_counts(ascending=False)\n",
    "rel_values = df_train['label'].value_counts(ascending=False, normalize=True).values * 100\n",
    "lbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n",
    "\n",
    "ax.bar_label(container=ax.containers[0], labels=lbls)\n",
    "\n",
    "ax.set_yticklabels([RESULT_DICT[index] for index in abs_values.index])\n",
    "\n",
    "ax.set_title(\"Distribution of labels in the training set\")"
   ],
   "id": "21cd13249c2b7fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.despine()\n",
    "ax = sns.countplot(data=df_train,\n",
    "                   y=\"language\",\n",
    "                   order = df_train['language'].value_counts().index)\n",
    "\n",
    "abs_values = df_train['language'].value_counts(ascending=False)\n",
    "rel_values = df_train['language'].value_counts(ascending=False, normalize=True).values * 100\n",
    "lbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n",
    "\n",
    "ax.bar_label(container=ax.containers[0], labels=lbls)\n",
    "\n",
    "ax.set_title(\"Distribution of languages in the training set\")"
   ],
   "id": "c53dbec795da2826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train[\"premise_length\"] = df_train[\"premise\"].apply(lambda x : len(x))\n",
    "df_train[\"hypothesis_length\"] = df_train[\"hypothesis\"].apply(lambda x : len(x))\n",
    "df_train[[\"hypothesis_length\", \"premise_length\"]].describe()"
   ],
   "id": "1914ad25c958b983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "TRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ],
   "id": "96f71659e57f231e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_labels(x, y):\n",
    "    return (x[0], x[1]), y\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        df_train[['premise', 'hypothesis']].values,\n",
    "        keras.utils.to_categorical(df_train['label'], num_classes=3).astype('float32')\n",
    "    )\n",
    ")\n",
    "\n",
    "train_dataset = training_dataset.take(TRAIN_SIZE)\n",
    "val_dataset = training_dataset.skip(TRAIN_SIZE)\n",
    "\n",
    "train_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .cache()\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .cache()\n",
    "        .prefetch(tf.data.AUTOTUNE)"
   ],
   "id": "5aecaabbd9dd0c92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"TRAIN SPEC:\", train_preprocessed.element_spec)\n",
    "print(\"VAL   SPEC:\", val_preprocessed.element_spec)"
   ],
   "id": "91fa1425259b3573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for (prem, hyp), labels in train_preprocessed.take(1):\n",
    "    print(\"Premise sample :\", prem[0].numpy()[:80], \"…\")\n",
    "    print(\"Hypothesis sample:\", hyp[0].numpy()[:80], \"…\")\n",
    "    print(\"Labels dtype    :\", labels.dtype)      # должно быть float32\n",
    "    print(\"Labels shape    :\", labels.shape)      # (batch, 3)\n",
    "    print(\"One-hot vector  :\", labels[0].numpy()) # например [0. 1. 0.]\n"
   ],
   "id": "2786e00622671d1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras import layers, optimizers, losses, metrics\n",
    "\n",
    "with strategy.scope():\n",
    "    classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_base_multi\", num_classes=3)\n",
    "\n",
    "    classifier.compile(optimizers.Adam(2e-5),\n",
    "                       losses.CategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=[metrics.CategoricalAccuracy()]\n",
    "                      )\n",
    "\n",
    "    classifier.summary()"
   ],
   "id": "26aaee74588f12b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "EPOCHS=3\n",
    "history = classifier.fit(train_preprocessed,\n",
    "                         epochs=EPOCHS,\n",
    "                         validation_data=val_preprocessed\n",
    "                        )"
   ],
   "id": "bf282d51b74691ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "predictions = classifier.predict((df_test['premise'],df_test['hypothesis']), batch_size=BATCH_SIZE)",
   "id": "991eea5151e8b579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "submission = df_test.id.copy().to_frame()\n",
    "submission[\"prediction\"] = np.argmax(predictions, axis=1)\n",
    "\n",
    "submission"
   ],
   "id": "f25d6ca544b085ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "submission.to_csv(\"submission.csv\", index=False)",
   "id": "8ba0f64ed3670cbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
